{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"DuyguTanima_Demo2.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"metadata":{"id":"0nQFIby0l74l","colab_type":"text"},"cell_type":"markdown","source":["# **Duygu & Cinsiyet Tanıma**\n","\n","\n","---\n","\n","\n","[<img align=\"right\" width=\"100\" height=\"100\" src=\"http://www.i2symbol.com/images/symbols/style-letters/circled_latin_capital_letter_a_u24B6_icon_128x128.png\">](https://www.ayyucekizrak.com/)"]},{"metadata":{"id":"4QRTRDhvqbs_","colab_type":"text"},"cell_type":"markdown","source":["**Colab** için kimlik doğrulama adımları:"]},{"metadata":{"id":"oLFsk1ELlIq4","colab_type":"code","colab":{}},"cell_type":"code","source":["!kill -9 -1"],"execution_count":0,"outputs":[]},{"metadata":{"id":"Rz4EI-ag7pjp","colab_type":"code","outputId":"006985cf-d68a-4c3a-d1dc-3ddccd9a300b","executionInfo":{"status":"ok","timestamp":1545644416086,"user_tz":-180,"elapsed":25744,"user":{"displayName":"ayyuce kizrak","photoUrl":"https://lh3.googleusercontent.com/-__1KfAaK-hQ/AAAAAAAAAAI/AAAAAAAAABA/D5feSxevpf4/s64/photo.jpg","userId":"05777443331456364044"}},"colab":{"base_uri":"https://localhost:8080/","height":125}},"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/gdrive')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/gdrive\n"],"name":"stdout"}]},{"metadata":{"id":"vnnM0bo594qs","colab_type":"code","outputId":"d0e655dc-7797-4fc6-e9c0-36bb0d5ac6a1","executionInfo":{"status":"ok","timestamp":1545644423321,"user_tz":-180,"elapsed":4931,"user":{"displayName":"ayyuce kizrak","photoUrl":"https://lh3.googleusercontent.com/-__1KfAaK-hQ/AAAAAAAAAAI/AAAAAAAAABA/D5feSxevpf4/s64/photo.jpg","userId":"05777443331456364044"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"cell_type":"code","source":["!ls '/content/gdrive/My Drive/Udemy_DerinOgrenmeyeGiris/Evrisimli_Sinir_Aglari/Duygu_Tanima/data/emotion_models/'"],"execution_count":0,"outputs":[{"output_type":"stream","text":["fer2013_mini_XCEPTION.110-0.65.hdf5\n"],"name":"stdout"}]},{"metadata":{"id":"WaSl3lUoql1A","colab_type":"text"},"cell_type":"markdown","source":["### Kütüphane kurulumları:"]},{"metadata":{"id":"ue60epE_OuF8","colab_type":"code","colab":{}},"cell_type":"code","source":["!apt-get -qq install -y libsm6 libxext6 && pip install -q -U opencv-python\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"7qa_qJK7O2RS","colab_type":"code","colab":{}},"cell_type":"code","source":["!pip install -q keras"],"execution_count":0,"outputs":[]},{"metadata":{"id":"loV0kI8Q-WU8","colab_type":"code","outputId":"0d1021bf-8377-4b39-941f-a3bc96ae57ef","executionInfo":{"status":"ok","timestamp":1545644441387,"user_tz":-180,"elapsed":4551,"user":{"displayName":"ayyuce kizrak","photoUrl":"https://lh3.googleusercontent.com/-__1KfAaK-hQ/AAAAAAAAAAI/AAAAAAAAABA/D5feSxevpf4/s64/photo.jpg","userId":"05777443331456364044"}},"colab":{"base_uri":"https://localhost:8080/","height":318}},"cell_type":"code","source":["!pip3 install --upgrade tensorflow"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Requirement already up-to-date: tensorflow in /usr/local/lib/python3.6/dist-packages (1.12.0)\n","Requirement already satisfied, skipping upgrade: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.0.6)\n","Requirement already satisfied, skipping upgrade: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.1.0)\n","Requirement already satisfied, skipping upgrade: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.15.0)\n","Requirement already satisfied, skipping upgrade: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (3.6.1)\n","Requirement already satisfied, skipping upgrade: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.2.0)\n","Requirement already satisfied, skipping upgrade: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.0.5)\n","Requirement already satisfied, skipping upgrade: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.14.6)\n","Requirement already satisfied, skipping upgrade: tensorboard<1.13.0,>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.12.1)\n","Requirement already satisfied, skipping upgrade: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.7.1)\n","Requirement already satisfied, skipping upgrade: absl-py>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.6.1)\n","Requirement already satisfied, skipping upgrade: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.32.3)\n","Requirement already satisfied, skipping upgrade: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.11.0)\n","Requirement already satisfied, skipping upgrade: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.6->tensorflow) (2.8.0)\n","Requirement already satisfied, skipping upgrade: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tensorflow) (40.6.3)\n","Requirement already satisfied, skipping upgrade: werkzeug>=0.11.10 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.13.0,>=1.12.0->tensorflow) (0.14.1)\n","Requirement already satisfied, skipping upgrade: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.13.0,>=1.12.0->tensorflow) (3.0.1)\n"],"name":"stdout"}]},{"metadata":{"id":"9qvCkAIXqriZ","colab_type":"text"},"cell_type":"markdown","source":["Drive da model dosyamızın olduğu klasöre gözatalım:"]},{"metadata":{"id":"b0kIklCfPHH4","colab_type":"code","colab":{}},"cell_type":"code","source":["import sys\n","sys.path.insert(0, '/content/gdrive/My Drive/Udemy_DerinOgrenmeyeGiris/Evrisimli_Sinir_Aglari/Duygu_Tanima/')"],"execution_count":0,"outputs":[]},{"metadata":{"id":"rXLCesK7qzbC","colab_type":"text"},"cell_type":"markdown","source":["**Gerekli paketleri yükleme...**"]},{"metadata":{"id":"1Y66dQsrN6w8","colab_type":"code","outputId":"b6f7e3b7-66f1-4877-d833-8c54cb2e45f7","executionInfo":{"status":"ok","timestamp":1545644451268,"user_tz":-180,"elapsed":5440,"user":{"displayName":"ayyuce kizrak","photoUrl":"https://lh3.googleusercontent.com/-__1KfAaK-hQ/AAAAAAAAAAI/AAAAAAAAABA/D5feSxevpf4/s64/photo.jpg","userId":"05777443331456364044"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"cell_type":"code","source":["import sys\n","\n","import cv2\n","from keras.models import load_model\n","from keras.preprocessing import image\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","from utils.datasets import get_labels\n","from utils.inference import detect_faces\n","from utils.inference import draw_text\n","from utils.inference import draw_bounding_box\n","from utils.inference import apply_offsets\n","from utils.inference import load_detection_model\n","from utils.inference import load_image\n","from utils.preprocessor import preprocess_input"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Using TensorFlow backend.\n"],"name":"stderr"}]},{"metadata":{"id":"0VskoiIx_P1f","colab_type":"code","outputId":"83e34095-05fe-465b-c71b-63b9dbedd830","executionInfo":{"status":"ok","timestamp":1545644285834,"user_tz":-180,"elapsed":2803,"user":{"displayName":"ayyuce kizrak","photoUrl":"https://lh3.googleusercontent.com/-__1KfAaK-hQ/AAAAAAAAAAI/AAAAAAAAABA/D5feSxevpf4/s64/photo.jpg","userId":"05777443331456364044"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"cell_type":"code","source":["!find . -name \"*.pyc\" -exec rm -f {} \\;"],"execution_count":0,"outputs":[{"output_type":"stream","text":["find: ‘./gdrive’: Transport endpoint is not connected\n"],"name":"stdout"}]},{"metadata":{"id":"aZgtf7NPq--0","colab_type":"text"},"cell_type":"markdown","source":["### Model ve veri kümesi yükleme işlemleri"]},{"metadata":{"id":"7ho7MxcqOrCD","colab_type":"code","colab":{}},"cell_type":"code","source":["root = 'gdrive/My Drive/Udemy_DerinOgrenmeyeGiris/Evrisimli_Sinir_Aglari/Duygu_Tanima/'\n","\n","# veri ve görüntüleri yüklemek için parametreler\n","# image_path = root + 'images/test.jpg'\n","\n","detection_model_path = root + 'data/detection_models/haarcascade_frontalface_default.xml'\n","emotion_model_path = root + 'data/emotion_models//fer2013_mini_XCEPTION.110-0.65.hdf5'\n","gender_model_path = root + 'data/gender_models/simple_CNN.81-0.96.hdf5'\n","emotion_labels = get_labels('fer2013')\n","gender_labels = get_labels('imdb')\n","font = cv2.FONT_HERSHEY_SIMPLEX\n","\n","# Çerçeve boyutu için hiperparametreler\n","gender_offsets = (30, 60)\n","gender_offsets = (10, 10)\n","emotion_offsets = (20, 40)\n","emotion_offsets = (0, 0)\n","\n","# Modelleri yükle\n","face_detection = load_detection_model(detection_model_path)\n","emotion_classifier = load_model(emotion_model_path, compile=False)\n","gender_classifier = load_model(gender_model_path, compile=False)\n","\n","# Duygu ve cinsiyet tespiti modellerinin giriş boyutları\n","emotion_target_size = emotion_classifier.input_shape[1:3]\n","gender_target_size = gender_classifier.input_shape[1:3]\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"cm-y2UFyPYIK","colab_type":"code","outputId":"6baa39b9-59ce-4b3f-cca1-b1c6bf909881","executionInfo":{"status":"error","timestamp":1545644470369,"user_tz":-180,"elapsed":1670,"user":{"displayName":"ayyuce kizrak","photoUrl":"https://lh3.googleusercontent.com/-__1KfAaK-hQ/AAAAAAAAAAI/AAAAAAAAABA/D5feSxevpf4/s64/photo.jpg","userId":"05777443331456364044"}},"colab":{"base_uri":"https://localhost:8080/","height":487}},"cell_type":"code","source":["# veri ve görüntüleri yüklemek için parametreler\n","image_path = root + 'images/test3.jpg'\n","\n","# Görüntüleri yükleme\n","rgb_image = load_image(image_path, grayscale=False)\n","gray_image = load_image(image_path, grayscale=True)\n","gray_image = np.squeeze(gray_image)\n","gray_image = gray_image.astype('uint8')\n","\n","\n","faces = detect_faces(face_detection, gray_image)\n","for face_coordinates in faces:\n","    x1, x2, y1, y2 = apply_offsets(face_coordinates, gender_offsets)\n","    rgb_face = rgb_image[y1:y2, x1:x2]\n","\n","    x1, x2, y1, y2 = apply_offsets(face_coordinates, emotion_offsets)\n","    gray_face = gray_image[y1:y2, x1:x2]\n","\n","    try:\n","        rgb_face = cv2.resize(rgb_face, (gender_target_size))\n","        gray_face = cv2.resize(gray_face, (emotion_target_size))\n","    except:\n","        continue\n","\n","    rgb_face = preprocess_input(rgb_face, False)\n","    rgb_face = np.expand_dims(rgb_face, 0)\n","    gender_prediction = gender_classifier.predict(rgb_face)\n","    gender_label_arg = np.argmax(gender_prediction)\n","    gender_text = gender_labels[gender_label_arg]\n","\n","    gray_face = preprocess_input(gray_face, True)\n","    gray_face = np.expand_dims(gray_face, 0)\n","    gray_face = np.expand_dims(gray_face, -1)\n","    emotion_label_arg = np.argmax(emotion_classifier.predict(gray_face))\n","    emotion_text = emotion_labels[emotion_label_arg]\n","\n","\n","    if emotion_text == 'angry':\n","        color = emotion_label_arg * np.asarray((255, 0, 0))\n","    elif emotion_text == 'sad':\n","        color = emotion_label_arg * np.asarray((0, 0, 255))\n","    elif emotion_text == 'happy':\n","        color = emotion_label_arg * np.asarray((255, 255, 0))\n","    elif emotion_text == 'surprise':\n","        color = emotion_label_arg * np.asarray((0, 255, 255))\n","    else:\n","        color = emotion_label_arg * np.asarray((0, 255, 0))\n","\n","    color = color.astype(int)\n","    color = color.tolist()\n","\n","    #if gender_text == gender_labels[0]:\n","    #    color = (0, 0, 255)\n","    #else:\n","    #    color = (255, 0, 0)\n","    \n","\n","    draw_bounding_box(face_coordinates, rgb_image, color)\n","    draw_text(face_coordinates, rgb_image, gender_text, color, 0, -20, 1, 2)\n","    draw_text(face_coordinates, rgb_image, emotion_text, color, 0, -50, 1, 2)\n","\n","bgr_image = cv2.cvtColor(rgb_image, cv2.COLOR_RGB2BGR)\n","cv2.imwrite(root + 'images/_predicted_image.jpg', bgr_image)\n","\n","print(\"Predicted! OK!\")\n","\n","predicted_image = image.load_img(root + 'images/_predicted_image.jpg')\n","\n","plt.axis('off')\n","plt.imshow(predicted_image)\n","plt.show()\n"],"execution_count":0,"outputs":[{"output_type":"error","ename":"ValueError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-9-2d29d4a11bdf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Görüntüleri yükleme\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mrgb_image\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrayscale\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mgray_image\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrayscale\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mgray_image\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgray_image\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/gdrive/My Drive/Udemy_DerinOgrenmeyeGiris/Evrisimli_Sinir_Aglari/Duygu_Tanima/utils/inference.py\u001b[0m in \u001b[0;36mload_image\u001b[0;34m(image_path, grayscale, target_size)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrayscale\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mpil_image\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_img\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrayscale\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimg_to_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpil_image\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras_preprocessing/image.py\u001b[0m in \u001b[0;36mload_img\u001b[0;34m(path, grayscale, color_mode, target_size, interpolation)\u001b[0m\n\u001b[1;32m    507\u001b[0m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'RGB'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    508\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 509\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'color_mode must be \"grayscale\", \"rbg\", or \"rgba\"'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    510\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtarget_size\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    511\u001b[0m         \u001b[0mwidth_height_tuple\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtarget_size\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_size\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: color_mode must be \"grayscale\", \"rbg\", or \"rgba\""]}]}]}